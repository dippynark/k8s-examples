# https://github.com/camilb/prometheus-kubernetes/blob/master/manifests/prometheus/prometheus-k8s-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: alert-rules
  name: alert-rules
  namespace: monitoring
spec:
  groups:
  - name: deadman
    rules:
    - alert: Deadman
      annotations:
        description: This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional.
      expr: vector(1)
      labels:
        severity: deadman
  - name: instance-down
    rules:
    - alert: KubeStateMetricsDown
      annotations:
        message: KubeStateMetrics has disappeared from Prometheus target discovery for 5m
      expr: absent(up{job="kube-state-metrics"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: NodeExporterUbntDown
      annotations:
        message: NodeExporter ubnt.lukeaddison.co.uk:9100 has disappeared from Prometheus target discovery for 5m
      expr: absent(up{job="node-exporter", instance="ubnt.lukeaddison.co.uk:9100"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: NodeExporterDaemonSetDown
      annotations:
        message: NodeExporter Daemonset has disappeared from Prometheus target discovery for 5m
      expr: absent(up{job="node-exporter", pod=~".+"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: BlackboxExporterDown
      annotations:
        message: BlackboxExporter has disappeared from Prometheus target discovery for 5m
      expr: absent(up{job="blackbox-exporter"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: FluentBitDaemonSetDown
      annotations:
        message: FluentBit Daemonset has disappeared from Prometheus target discovery for 5m
      expr: absent(up{job="fluent-bit", pod=~".+"} == 1)
      for: 5m
      labels:
        severity: critical
  - name: kubernetes-applications
    rules:
    - alert: KubePodNotRunning
      annotations:
        message: Pod {{ $labels.exported_namespace }}/{{ $labels.exported_pod }} has not been running for 5m
      expr: |
        sum by(exported_namespace, exported_pod) (kube_pod_status_phase{job="kube-state-metrics",phase!~"Running|Succeeded|Failed"}) > 0
      for: 5m
      labels:
        severity: critical
    - alert: KubeContainerNotReady
      annotations:
        message: Container {{ $labels.container }} in Pod {{ $labels.exported_namespace }}/{{ $labels.exported_pod }} has not been ready for 5m
      expr: |
        kube_pod_container_status_ready + on(exported_pod) group_left(phase) (0 * (kube_pod_status_phase{phase="Running"} > 0)) == 0
      for: 5m
      labels:
        severity: critical
    - alert: KubeContainerReadinessFlapping
      annotations:
        message: Container {{ $labels.container }} in Pod {{ $labels.exported_namespace }}/{{ $labels.exported_pod }} changing it's readiness status regularly for 10m
      expr: |
        changes(kube_pod_container_status_ready[5m]) > 0
      for: 10m
      labels:
        severity: critical
    - alert: KubeContainerCrashLooping
      annotations:
        message: Container {{ $labels.container }} in Pod {{ $labels.exported_namespace }}/{{ $labels.exported_pod }} has restarted regularly for 30m
      expr: |
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) > 0
      for: 30m
      labels:
        severity: critical
    - alert: KubeCronJobLastJobFailedAfterSuccess
      annotations:
        message: CronJob {{ $labels.exported_namespace }}/{{ $labels.owner_name }} is failing after a previous success
      expr: |
        (max(kube_job_status_start_time * on(job_name, exported_namespace) group_left(owner_name) ((kube_job_status_succeeded/kube_job_status_succeeded == 1) + on(job_name, exported_namespace) group_left(owner_name) (0 * kube_job_owner{owner_kind="CronJob", owner_is_controller="true"}))) by (owner_name, exported_namespace))
        < bool
        (max(kube_job_status_start_time * on(job_name, exported_namespace) group_left(owner_name) ((kube_job_status_failed/kube_job_status_failed == 1) + on(job_name, exported_namespace) group_left(owner_name) (0 * kube_job_owner{owner_kind="CronJob", owner_is_controller="true"}))) by (owner_name, exported_namespace)) == 1
      for: 1m
      labels:
        severity: critical
    - alert: KubeCronJobNoSucceededJobs
      annotations:
        message: CronJob {{ $labels.exported_namespace }}/{{ $labels.owner_name }} has no succeeded jobs
      expr: |
        ((sum((kube_job_created > bool 0) + on(job_name, exported_namespace) group_left(owner_name) (0 * kube_job_owner{owner_is_controller="true",owner_kind="CronJob"})) by (owner_name,exported_namespace) > bool 0)
        -
        (sum((kube_job_status_succeeded > bool 0) + on(job_name, exported_namespace) group_left(owner_name) (0 * kube_job_owner{owner_is_controller="true",owner_kind="CronJob"})) by (owner_name,exported_namespace) > bool 0)) == 1
      labels:
        severity: critical
    - alert: KubeJobStillRunning
      annotations:
        message: Job {{ $labels.exported_namespace }}/{{ $labels.job_name }} has been running for an hour
      expr: |
        kube_job_status_active == 1 and time() - kube_job_status_start_time > 60*60
      labels:
        severity: critical
  - name: kubernetes-system
    rules:
    - alert: KubeNodeNotReady
      annotations:
        message: Node {{ $labels.node }} has been unready for 5m
      expr: |
        kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
    - alert: KubeCronJobInvalidNextScheduleTime
      annotations:
        message: CronJob {{ $labels.exported_namespace }}/{{ $labels.owner_name }} next schedule time has been in the past for 10m
      expr: |
        kube_cronjob_next_schedule_time - time() < 0
      for: 10m
      labels:
        severity: critical
  - name: node-systemd-unit-state
    rules:
    - alert: NodeSystemdUnitStateFailed
      annotations:
        message: Systemd unit {{ $labels.name }} entered failed state on instance {{ $labels.instance }} for 5m
      expr: |
        node_systemd_unit_state{state="failed"} == 1
      for: 5m
      labels:
        severity: critical
    - alert: NodeSystemdUnitStateRestarting
      annotations:
        message: Systemd unit {{ $labels.name }} on instance {{ $labels.instance }} has restarted regularly for 10m
      expr: |
        rate(node_systemd_service_restart_total[5m]) > 0
      for: 10m
      labels:
        severity: critical
  - name: node-disk
    rules:
    - alert: NodeDiskUsageHigh
      annotations:
        message: Filesystem at {{ $labels.mountpoint }} on instance {{ $labels.instance }} has less than 5% available space
      expr: |
        (node_filesystem_avail_bytes*100)/node_filesystem_size_bytes < 5
      labels:
        severity: critical
  - name: node-memory
    rules:
    - alert: NodeMemoryUsageHigh
      annotations:
        message: Instance {{ $labels.instance }} has over 90% memory utilisation
      expr: |
        (node_memory_MemTotal_bytes-node_memory_MemFree_bytes-node_memory_Cached_bytes)/node_memory_MemTotal_bytes*100 > 90
      labels:
        severity: critical
  - name: node-network
    rules:
    - alert: NetworkReceiveErrors
      annotations:
        message: Network interface {{ $labels.device }} in Pod {{ $labels.namespace }}/{{ $labels.pod }} showing receive errors
      expr: rate(node_network_receive_errs_total{job="node-exporter",device!~"veth.+"}[2m]) > 0
      for: 2m
      labels:
        severity: warning
    - alert: NetworkTransmitErrors
      annotations:
        message: Network interface {{ $labels.device }} in Pod {{ $labels.namespace }}/{{ $labels.pod }} showing transmit errors
      expr: rate(node_network_transmit_errs_total{job="node-exporter",device!~"veth.+"}[2m]) > 0
      for: 2m
      labels:
        severity: warning
    - alert: NodeNetworkInterfaceFlapping
      annotations:
        message: Network interface {{ $labels.device }} in Pod {{ $labels.namespace }}/{{ $labels.pod }} changing it's up status often
      expr: changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
  - name: probe-failed
    rules:
    - alert: ProbeFailed
      annotations:
        message: '{{ $labels.target }} has been unavailable for 2m'
      expr: |
        probe_success == 0
      for: 2m
      labels:
        severity: critical
